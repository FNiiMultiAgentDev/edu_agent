import json
import base64
import os
from pathlib import Path
from openai import OpenAI
from typing import Tuple, List, Dict, Any
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from statistics import median
from collections import defaultdict
import datetime
import copy
import numpy as np

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

class EduAgent:
    def __init__(self, 
                 students_root: str = "students", 
                 questions_root: str = "questions",
                 exam_id: str = "exam_5",
                 grade: str = "é«˜ä¸‰",
                 class_id: str = "5ç­",
                 paper_id: str = "ç¬¬äºŒå­¦æœŸ ç¬¬äº”æ¬¡è€ƒè¯•",
                 current_exam_number: int = 5,
                 openai_api_key: str = os.getenv("DASHSCOPE_API_KEY"),
                 openai_base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1"):
        self.students_root = students_root
        self.questions_root = questions_root
        self.exam_id = exam_id
        self.grade = grade
        self.class_id = class_id
        self.paper_id = paper_id
        self.current_exam_number = current_exam_number
        self.openai_api_key = openai_api_key
        self.openai_base_url = openai_base_url
        self.client = OpenAI(
            api_key=self.openai_api_key,
            base_url=self.openai_base_url
        )

    # ========== è¯„åˆ†ç›¸å…³ ==========
    def scoring_single_choice(self,question_id: str, student_s_answer: str)-> Tuple[int, str, bool]:
        """
        ä¸ºå•é¡¹é€‰æ‹©é¢˜è¿›è¡Œè¯„åˆ†
        
        Args:
            question_id (str): é¢˜ç›®IDï¼Œç”¨äºå®šä½é¢˜ç›®å…ƒæ•°æ®æ–‡ä»¶ã€‚é»˜è®¤å€¼ä¸º "1"
            student_answer (str): å­¦ç”Ÿçš„ç­”æ¡ˆé€‰é¡¹ï¼ˆå¦‚ "A", "B", "C", "D"ï¼‰ã€‚é»˜è®¤å€¼ä¸º "A"
        
        Returns:
            Tuple[int, str, bool]: è¿”å›ä¸‰å…ƒç»„
                - point_earned: å­¦ç”Ÿè·å¾—çš„åˆ†æ•°
                - question_focus: é¢˜ç›®è€ƒæŸ¥é‡ç‚¹/çŸ¥è¯†ç‚¹
                - get_full_point: æ˜¯å¦è·å¾—æ»¡åˆ†ï¼ˆå¸ƒå°”å€¼ï¼‰
        """
        question_metadata_path = f"questions/{question_id}/question_metadata.json"
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            correct_answer = json.load(question_metadata)["correct_answer"]
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            full_point = json.load(question_metadata)["full_point"]
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            question_focus = json.load(question_metadata)["question_focus"]
        if correct_answer == student_s_answer:
            point_earned = full_point
        else:
            point_earned = 0
        get_full_point = (point_earned == full_point)
        return point_earned, question_focus, get_full_point

    def scoring_multiple_choice(self,question_id = str, student_s_answer = str)-> Tuple[int, str, bool]:
        """
        ä¸ºå¤šé¡¹é€‰æ‹©é¢˜è¿›è¡Œè¯„åˆ†
        
        Args:
            question_id (str): é¢˜ç›®IDï¼Œç”¨äºå®šä½é¢˜ç›®å…ƒæ•°æ®æ–‡ä»¶ã€‚é»˜è®¤å€¼ä¸º "9"
            student_answer (str): å­¦ç”Ÿçš„ç­”æ¡ˆé€‰é¡¹ï¼ˆå¦‚ "ABC", "B", "C", "D"ï¼‰ã€‚é»˜è®¤å€¼ä¸º "A"
        
        Returns:
            Tuple[int, str, bool]: è¿”å›ä¸‰å…ƒç»„
                - point_earned: å­¦ç”Ÿè·å¾—çš„åˆ†æ•°
                - question_focus: é¢˜ç›®è€ƒæŸ¥é‡ç‚¹/çŸ¥è¯†ç‚¹
                - get_full_point: æ˜¯å¦è·å¾—æ»¡åˆ†ï¼ˆå¸ƒå°”å€¼ï¼‰
        """
        question_metadata_path = f"questions/{question_id}/question_metadata.json"
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            correct_answer = json.load(question_metadata)["correct_answer"]
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            full_point = json.load(question_metadata)["full_point"]
        with open(question_metadata_path,
                "r", encoding="utf-8") as question_metadata:
            partial_correct_point = json.load(question_metadata)["scoring_rules"]["partially_correct"]
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            question_focus = json.load(question_metadata)["question_focus"]
        if correct_answer == student_s_answer:
            point_earned = full_point
        elif not (set(["A", "B", "C", "D"]) - set(list(correct_answer))) & set(list(student_s_answer)):
            point_earned = partial_correct_point
        else:
            point_earned = 0
        get_full_point = (point_earned == full_point)
        return point_earned, question_focus, get_full_point

    def scoring_fill_in_blank(self,question_id = str, student_s_answer = list[str])-> Tuple[int, str, bool]:
        """
        ä¸ºå¡«ç©ºé¢˜è¿›è¡Œè¯„åˆ†
        
        Args:
            question_id (str): é¢˜ç›®IDï¼Œç”¨äºå®šä½é¢˜ç›®å…ƒæ•°æ®æ–‡ä»¶ã€‚é»˜è®¤å€¼ä¸º "12"
            student_answer (str): å­¦ç”Ÿçš„ç­”æ¡ˆ
        
        Returns:
            Tuple[int, str, bool]: è¿”å›ä¸‰å…ƒç»„
                - point_earned: å­¦ç”Ÿè·å¾—çš„åˆ†æ•°
                - question_focus: é¢˜ç›®è€ƒæŸ¥é‡ç‚¹/çŸ¥è¯†ç‚¹
                - get_full_point: æ˜¯å¦è·å¾—æ»¡åˆ†ï¼ˆå¸ƒå°”å€¼ï¼‰
        """
        def compare_difference(correct_answer: list, student_s_answer: list)-> np.array:
            """
            ç»Ÿè®¡åˆ—è¡¨student_s_answerä¸­ä¸correct_answerå¯¹åº”ä½ç½®å…ƒç´ ä¸åŒçš„æ•°é‡

            å‚æ•°:
                correct_answer (list): åˆ—è¡¨å­˜å‚¨çš„å‚è€ƒç­”æ¡ˆ
                student_s_answer (list): åˆ—è¡¨å­˜å‚¨çš„å­¦ç”Ÿç­”æ¡ˆ

            è¿”å›:
                np.array: æ¯ä¸ªä½ç½®æ˜¯å¦å…ƒç´ ç›¸ç­‰
            """
            if len(correct_answer) != len(student_s_answer):
                raise ValueError("æå–å‡ºçš„åˆ—è¡¨å­˜å‚¨çš„å­¦ç”Ÿç­”æ¡ˆé•¿åº¦å’Œå‚è€ƒç­”æ¡ˆåˆ—è¡¨é•¿åº¦ä¸åŒ¹é…ï¼Œè½¬äº¤ç»™å¤§æ¨¡å‹æ‰¹æ”¹ï¼Œè¯·ç¨ç­‰")
            return np.array([a != b for a, b in zip(correct_answer, student_s_answer)])
        question_metadata_path = f"questions/{question_id}/question_metadata.json"
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            correct_answer = json.load(question_metadata)["correct_answer"]
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            scoring_rules = json.load(question_metadata)["scoring_rules"]
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            full_point = json.load(question_metadata)["full_point"]
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            question_focus = json.load(question_metadata)["question_focus"]
        difference_comparison_between_correct_answer_and_student_s_answer = compare_difference(correct_answer,student_s_answer)
        point_earned = full_point + difference_comparison_between_correct_answer_and_student_s_answer@np.array(scoring_rules)
        point_earned = int(point_earned)    
        get_full_point = (point_earned == full_point)
        return point_earned, question_focus, get_full_point

    def scoring_comprehensive_problems(self,question_id: str, student_s_answer: str)-> Tuple[int, str, bool, str]:
        """
        ä¸ºè§£ç­”é¢˜è¿›è¡Œè¯„åˆ†
        
        Args:
            question_id (str): é¢˜ç›®IDï¼Œç”¨äºå®šä½é¢˜ç›®å…ƒæ•°æ®æ–‡ä»¶ã€‚
            student_answer (str): å­¦ç”Ÿçš„ç­”æ¡ˆ
        
        Returns:
            Tuple[int, str, bool, str]: è¿”å›å››å…ƒç»„
                - point_earned: å­¦ç”Ÿè·å¾—çš„åˆ†æ•°
                - question_focus: é¢˜ç›®è€ƒæŸ¥é‡ç‚¹/çŸ¥è¯†ç‚¹
                - get_full_point: æ˜¯å¦è·å¾—æ»¡åˆ†ï¼ˆå¸ƒå°”å€¼ï¼‰
                - LLM_feedback: å¤§æ¨¡å‹æ‰¹æ”¹ç»“æœ
        """
        question_metadata_path = f"questions/{question_id}/question_metadata.json"
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            question = json.load(question_metadata)["question"]
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            full_point = json.load(question_metadata)["full_point"]
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            question_focus = json.load(question_metadata)["question_focus"]
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            grading_rubric = json.load(question_metadata)["grading_rubric"]
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            few_shot = json.load(question_metadata)["few_shot"]

        client = OpenAI(
            api_key="sk-9645af39a0e44b1fa14e73d68a4b9b68",
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )

        completion = client.chat.completions.create(
            model="qwen3-32b",
            messages=[
                {
                    "role": "system",
                    "content": f"""ç°åœ¨ä½ æ˜¯ä¸€ä¸ªä¸­å­¦è€å¸ˆï¼Œä½ è¦è´Ÿè´£æ‰¹æ”¹ä½ å­¦ç”Ÿçš„æ•°å­¦è¯•å·çš„é¢˜ç›®ï¼Œ
                    ä¸»ä»»è¦æ±‚ä½ ä¸¥æ ¼æŒ‰ç…§é¢˜ç›®è§£æä¸å„ç‚¹è¯„åˆ†æ ‡å‡†é‡Œçš„è¸©åˆ†ç‚¹æ¥è¿›è¡Œæ‰¹æ”¹å¾—åˆ†ï¼Œ
                    å¹¶å¯¹æ¯ä¸€ä¸ªè¸©åˆ†ç‚¹è¿›è¡Œè§£é‡Šï¼Œæ¯”å¦‚è¯´è¿™ä¸ªè¸©åˆ†ç‚¹æœ‰å¯¹åº”çš„å…¬å¼ï¼Œå¾—åˆ°ç›¸åº”çš„åˆ†æ•°ï¼Œä¸æ˜¯ç´¯è®¡å¾—åˆ†ï¼ï¼Œ
                    é‚£ä¸ªè¸©åˆ†ç‚¹æ²¡æœ‰å…¬å¼æˆ–è€…å…¬å¼é”™è¯¯ï¼Œä¸å¾—åˆ†"ã€‚æŒ‰ç…§æ‰¹æ”¹ç¤ºä¾‹é‡Œçš„"few_shot_outputâ€çš„jsonæ ¼å¼è¿”å›ä¸€ä¸ª{{LLM_feedback:{{"correction_and_explanation":{{}}}},"point_earned_of_this_question":}}ï¼Œéµå¾ªfew_shot_outputé‡Œçš„ç»™åˆ†ç‚¹:
                    é¢˜ç›®ï¼š{question}
                    è¯„åˆ†ç»†åˆ™:{grading_rubric}
                    æ‰¹æ”¹ç¤ºä¾‹ï¼š{few_shot}
                """
                },
                {
                    "role": "user",
                    "content": f"å­¦ç”Ÿç­”æ¡ˆï¼š{student_s_answer}", 
                },
            ],
            temperature= 0.3,
            response_format={"type": "json_object"},
            extra_body={"enable_thinking": False}
        )

        json_string = completion.choices[0].message.content
        json_string = json.loads(json_string)
        print(json_string)

        LLM_feedback = json_string["LLM_feedback"]
        point_earned = json_string["LLM_feedback"]["point_earned_of_this_question"]
        get_full_point = (point_earned == full_point)
        
        return point_earned, question_focus, get_full_point, LLM_feedback
    
    def scoring_by_LLM_without_answer(self,question_id: str, student_s_answer: str):
        """
        å¤§æ¨¡å‹åˆ¤æ–­å­¦ç”Ÿç­”æ¡ˆæ˜¯å¦æ­£ç¡®çš„å‡½æ•°ï¼Œé€‚ç”¨äºåªç¼ºç­”æ¡ˆå’Œè¯„åˆ†æ ‡å‡†çš„æƒ…å†µ
        
        Args:
            question_id (str): é¢˜ç›®IDï¼Œç”¨äºå®šä½é¢˜ç›®å…ƒæ•°æ®æ–‡ä»¶ã€‚
            student_answer (str): å­¦ç”Ÿçš„ç­”æ¡ˆ
        
        Returns:
            Tuple[int, str, bool, str]: è¿”å›å››å…ƒç»„
                - point_earned: å­¦ç”Ÿè·å¾—çš„åˆ†æ•°
                - question_focus: é¢˜ç›®è€ƒæŸ¥é‡ç‚¹/çŸ¥è¯†ç‚¹
                - get_full_point: æ˜¯å¦è·å¾—æ»¡åˆ†ï¼ˆå¸ƒå°”å€¼ï¼‰
                - LLM_feedback: å¤§æ¨¡å‹æ‰¹æ”¹ç»“æœ
        """
        question_metadata_path = f"questions/{question_id}/question_metadata.json"
        
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            question = json.load(question_metadata)["question"]
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            question_focus = json.load(question_metadata)["question_focus"]
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            full_point = json.load(question_metadata)["full_point"]
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            section_it_belongs_to = json.load(question_metadata)["section_it_belongs_to"]
        scoring_rules_given_question_section_path = f"scoring_rules_given_question_section/{section_it_belongs_to}.json"
        with open(scoring_rules_given_question_section_path, "r", encoding="utf-8") as scoring_rules_given_question_section:
            scoring_rules = json.load(scoring_rules_given_question_section)["scoring_description"]
        client = OpenAI(
            api_key="sk-9645af39a0e44b1fa14e73d68a4b9b68",
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )

        completion = client.chat.completions.create(
            model="qwen3-32b",
            messages=[
                {
                    "role": "system",
                    "content": f"""ç°åœ¨ä½ æ˜¯ä¸€ä¸ªé«˜ä¸­è€å¸ˆï¼Œä½ è¦è´Ÿè´£æ‰¹æ”¹ä½ å­¦ç”Ÿçš„æ•°å­¦è¯•å·çš„é¢˜ç›®ï¼Œ
                    ä¸»ä»»è¦æ±‚ä½ åˆ¤æ–­å­¦ç”Ÿçš„ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼Œåº”è¯¥ç»™å‡ åˆ†  
                    é¢˜ç›®ï¼š{question}
                    
                """
                },
                {
                    "role": "user",
                    "content": f"å­¦ç”Ÿç­”æ¡ˆï¼š{student_s_answer}", 
                },
            ],
            temperature= 0.3,
            extra_body={"enable_thinking": True},
            stream=True,
        
        )
        reasoning_content = ""  # å®Œæ•´æ€è€ƒè¿‡ç¨‹
        answer_content = ""  # å®Œæ•´å›å¤
        is_answering = False  # æ˜¯å¦è¿›å…¥å›å¤é˜¶æ®µ
        #print("\n" + "=" * 20 + "æ€è€ƒè¿‡ç¨‹" + "=" * 20 + "\n")

        for chunk in completion:
            if not chunk.choices:
                print("\nUsage:")
                print(chunk.usage)
                continue

            delta = chunk.choices[0].delta

            # åªæ”¶é›†æ€è€ƒå†…å®¹
            if hasattr(delta, "reasoning_content") and delta.reasoning_content is not None:
                if not is_answering:
                    #print(delta.reasoning_content, end="", flush=True)
                    reasoning_content += delta.reasoning_content

            # æ”¶åˆ°contentï¼Œå¼€å§‹è¿›è¡Œå›å¤
            if hasattr(delta, "content") and delta.content:
                if not is_answering:
                    #print("\n" + "=" * 20 + "å®Œæ•´å›å¤" + "=" * 20 + "\n")
                    is_answering = True
                #print(delta.content, end="", flush=True)
                answer_content += delta.content
        LLM_feedback = "å¤§æ¨¡å‹è¯„åˆ†:"+answer_content+"\n\næ€è€ƒè¿‡ç¨‹ï¼š" + reasoning_content
        completion = client.chat.completions.create(
            model="qwen3-32b",
            messages=[
                {
                    "role": "system",
                    "content": f"""ç°åœ¨ä½ æ˜¯ä¸€ä¸ªé«˜ä¸­è€å¸ˆï¼Œä½ è¦è´Ÿè´£æ‰¹æ”¹ä½ å­¦ç”Ÿçš„æ•°å­¦è¯•å·çš„é¢˜ç›®ï¼Œ
                è¿™æ˜¯å¤§æ¨¡å‹å¯¹äºå­¦ç”Ÿè¿™é“é¢˜ç­”é¢˜æƒ…å†µçš„åˆ†æï¼š{LLM_feedback}ã€‚
                è¿™æ˜¯è¿™é“é¢˜æ»¡åˆ†{full_point}åˆ†ï¼Œè¯·ä½ åˆ¤æ–­è¿™é“é¢˜å­¦ç”Ÿåº”è¯¥å¾—å‡ åˆ†ï¼Œè¿”å›å¦‚ä¸‹jsonæ ¼å¼ï¼š{{"point_earned": <å­¦ç”Ÿè·å¾—çš„åˆ†æ•°,int>, "get_full_point": <æ˜¯å¦è·å¾—æ»¡åˆ†,bool:True/False>, }}ã€‚  
                
                """
                },
                {
                    "role": "user",
                    "content": f"å­¦ç”Ÿç­”æ¡ˆï¼š{student_s_answer}", 
                },
            ],
            temperature= 0.3,
            extra_body={"enable_thinking": False},
            response_format={"type": "json_object"},
        )
        print(completion.choices[0].message.content)
        print(type(completion.choices[0].message.content))
        point_earned = json.loads(completion.choices[0].message.content)["point_earned"]
        get_full_point = json.loads(completion.choices[0].message.content)["get_full_point"]

        return point_earned, question_focus, get_full_point, LLM_feedback
    
    def merely_get_wrong_or_correct_from_LLM(self,question_id: str, student_s_answer: str):
        """
        å¤§æ¨¡å‹åˆ¤æ–­å­¦ç”Ÿç­”æ¡ˆæ˜¯å¦æ­£ç¡®çš„å‡½æ•°ï¼Œé€‚ç”¨äºåªæœ‰é¢˜ç›®çš„æƒ…å†µ
        
        Args:
            question_id (str): é¢˜ç›®IDï¼Œç”¨äºå®šä½é¢˜ç›®å…ƒæ•°æ®æ–‡ä»¶ã€‚
            student_answer (str): å­¦ç”Ÿçš„ç­”æ¡ˆ
        
        Returns:
            LLM_feedback (str): å¤§æ¨¡å‹æ‰¹æ”¹ç»“æœ
        """
        question_metadata_path = f"questions/{question_id}/question_metadata.json"
        with open(question_metadata_path, "r", encoding="utf-8") as question_metadata:
            question = json.load(question_metadata)["question"]

        client = OpenAI(
            api_key="sk-9645af39a0e44b1fa14e73d68a4b9b68",
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )

        completion = client.chat.completions.create(
            model="qwen3-32b",
            messages=[
                {
                    "role": "system",
                    "content": f"""ç°åœ¨ä½ æ˜¯ä¸€ä¸ªä¸­å­¦è€å¸ˆï¼Œä½ è¦è´Ÿè´£æ‰¹æ”¹ä½ å­¦ç”Ÿçš„æ•°å­¦è¯•å·çš„é¢˜ç›®ï¼Œ
                    ä¸»ä»»è¦æ±‚ä½ åˆ¤æ–­å­¦ç”Ÿçš„ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ã€‚  
                    é¢˜ç›®ï¼š{question}
                """
                },
                {
                    "role": "user",
                    "content": f"å­¦ç”Ÿç­”æ¡ˆï¼š{student_s_answer}", 
                },
            ],
            temperature= 0.3,
            extra_body={"enable_thinking": True},
            stream=True,
        
        )
        reasoning_content = ""  # å®Œæ•´æ€è€ƒè¿‡ç¨‹
        answer_content = ""  # å®Œæ•´å›å¤
        is_answering = False  # æ˜¯å¦è¿›å…¥å›å¤é˜¶æ®µ
        #print("\n" + "=" * 20 + "æ€è€ƒè¿‡ç¨‹" + "=" * 20 + "\n")

        for chunk in completion:
            if not chunk.choices:
                print("\nUsage:")
                print(chunk.usage)
                continue

            delta = chunk.choices[0].delta

            # åªæ”¶é›†æ€è€ƒå†…å®¹
            if hasattr(delta, "reasoning_content") and delta.reasoning_content is not None:
                if not is_answering:
                    #print(delta.reasoning_content, end="", flush=True)
                    reasoning_content += delta.reasoning_content

            # æ”¶åˆ°contentï¼Œå¼€å§‹è¿›è¡Œå›å¤
            if hasattr(delta, "content") and delta.content:
                if not is_answering:
                    #print("\n" + "=" * 20 + "å®Œæ•´å›å¤" + "=" * 20 + "\n")
                    is_answering = True
                #print(delta.content, end="", flush=True)
                answer_content += delta.content
        LLM_feedback = "å¤§æ¨¡å‹è¯„åˆ†"+answer_content+"\n\næ€è€ƒè¿‡ç¨‹ï¼š" + reasoning_content
            
        print(LLM_feedback)

        return LLM_feedback
    # ========== å­¦ç”Ÿç­”æ¡ˆæå– ==========
    def student_s_answers_extractor(self,student_id = 2024001):
        def save_json_string_to_file(json_string, file_path):
            """
            å°†JSONå­—ç¬¦ä¸²ä¿å­˜ä¸ºæœ¬åœ°JSONæ–‡ä»¶
            
            Args:
                json_string (str): JSONæ ¼å¼çš„å­—ç¬¦ä¸²
                file_path (str): è¦ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
            """
            try:
                # é¦–å…ˆéªŒè¯JSONå­—ç¬¦ä¸²æ˜¯å¦æœ‰æ•ˆ
                json_data = json.loads(json_string)
                
                # å†™å…¥æ–‡ä»¶ï¼Œä½¿ç”¨ensure_ascii=Falseæ”¯æŒä¸­æ–‡å­—ç¬¦
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(json_data, f, ensure_ascii=False, indent=2)
                
                print(f"JSONæ–‡ä»¶å·²æˆåŠŸä¿å­˜åˆ°: {file_path}")
                
            except json.JSONDecodeError as e:
                print(f"JSONæ ¼å¼é”™è¯¯: {e}")
            except Exception as e:
                print(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    #  base 64 ç¼–ç æ ¼å¼
        def encode_image(image_path):
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode("utf-8")

        base64_image_1 = encode_image(f"../edu_agent/students/{student_id}/answered_paper_sheet_scan/1.jpg")
        base64_image_2 = encode_image(f"../edu_agent/students/{student_id}/answered_paper_sheet_scan/2.jpg")
        base64_image_3 = encode_image(f"../edu_agent/students/{student_id}/answered_paper_sheet_scan/3.jpg")

        client = OpenAI(
            api_key=os.getenv('DASHSCOPE_API_KEY'),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
        few_shot = {"å•é¡¹é€‰æ‹©é¢˜":{"1":"A","2":"B"},"å¤šé¡¹é€‰æ‹©é¢˜":{"9":"ABC"},"å¡«ç©ºé¢˜":{"12":[""],"13":[""],"14":[""]},"è§£ç­”é¢˜":{"15":"å¿ å®è½¬å½•å­¦ç”Ÿçš„æ‰‹å†™è§£é¢˜è¿‡ç¨‹"}}
        student_s_answers = client.chat.completions.create(
            model="qwen-vl-max-latest", 
            messages=[
                {
                    "role": "system",
                    "content": [{"type":"text","text":f"ä½ æ˜¯ä¸€å°ä½œä¸šæ‰«ææœºï¼Œè¯·ä½ æŒ‰ç…§é¡ºåºæŠŠå­¦ç”Ÿæ‰‹å†™çš„ç­”æ¡ˆæå–ä¸ºè¿™æ ·çš„æ ¼å¼ï¼š{few_shot}ï¼Œä¸è¦ç¯¡æ”¹ã€æ·»åŠ ã€åˆ å‡å­¦ç”Ÿçš„æ‰‹å†™å†…å®¹"}]},
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/png;base64,{base64_image_1}"}, 
                        },
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/png;base64,{base64_image_2}"}, 
                        },
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/png;base64,{base64_image_3}"}, 
                        },
                    ],
                }
            ],
            response_format={"type": "json_object"},
        )
        print(student_s_answers.choices[0].message.content)
        save_json_string_to_file(student_s_answers.choices[0].message.content, file_path = f"../edu_agent/students/{student_id}/student_s_answers.json")
        return student_s_answers.choices[0].message.content

    # ========== æ‰¹æ”¹æ‰§è¡Œ ==========
    def scoring_executor(self,student_id = 2024001):
        # è¯»å–JSONæ–‡ä»¶
        student_answer_revise_path = f"../edu_agent/students/{student_id}/answer_revise.json"
        student_s_answers_path = f"../edu_agent/students/{student_id}/student_s_answers.json"
        with open(student_answer_revise_path, 'r', encoding='utf-8') as file:
            answer_revise = json.load(file)
        with open(student_s_answers_path, 'r', encoding='utf-8') as file:
            student_s_answers = json.load(file)
        for idx, question_id in enumerate(student_s_answers["å•é¡¹é€‰æ‹©é¢˜"]):
            LLM_feedback = ""
            with open(f"../edu_agent/questions/{question_id}/question_metadata.json", 'r', encoding='utf-8') as file:
                correct_answer = json.load(file)["correct_answer"]
            with open(f"../edu_agent/questions/{question_id}/question_metadata.json", 'r', encoding='utf-8') as file:
                grading_rubric = json.load(file).get("grading_rubric")
            if (correct_answer != None) | (grading_rubric != None):
                point_earned, question_focus, get_full_point = self.scoring_single_choice(question_id, student_s_answers["å•é¡¹é€‰æ‹©é¢˜"][question_id])
            else:
                point_earned, question_focus, get_full_point, LLM_feedback = self.scoring_by_LLM_without_answer(question_id, student_s_answers["å•é¡¹é€‰æ‹©é¢˜"][question_id])
            # ä¿®æ”¹ç°æœ‰é”®çš„å€¼
            answer_revise[question_id] = {"point_earned" : point_earned,
                                        "question_focus" : question_focus,
                                        "get_full_point" : get_full_point,
                                        "LLM_feedback": LLM_feedback
                                        }
            
        for idx, question_id in enumerate(student_s_answers["å¤šé¡¹é€‰æ‹©é¢˜"]):
            LLM_feedback = ""
            with open(f"../edu_agent/questions/{question_id}/question_metadata.json", 'r', encoding='utf-8') as file:
                correct_answer = json.load(file)["correct_answer"]
            with open(f"../edu_agent/questions/{question_id}/question_metadata.json", 'r', encoding='utf-8') as file:
                grading_rubric = json.load(file).get("grading_rubric")
            if (correct_answer != None) | (grading_rubric != None):
                point_earned, question_focus, get_full_point = self.scoring_multiple_choice(question_id, student_s_answers["å¤šé¡¹é€‰æ‹©é¢˜"][question_id])
            else:
                point_earned, question_focus, get_full_point, LLM_feedback = self.scoring_by_LLM_without_answer(question_id, student_s_answers["å¤šé¡¹é€‰æ‹©é¢˜"][question_id])
            # ä¿®æ”¹ç°æœ‰é”®çš„å€¼
            answer_revise[question_id] = {"point_earned" : point_earned,
                                        "question_focus" : question_focus,
                                        "get_full_point" : get_full_point,
                                        "LLM_feedback": LLM_feedback
                                        }
            
        for idx, question_id in enumerate(student_s_answers["å¡«ç©ºé¢˜"]):
            LLM_feedback = ""
            with open(f"../edu_agent/questions/{question_id}/question_metadata.json", 'r', encoding='utf-8') as file:
                correct_answer = json.load(file)["correct_answer"]
            with open(f"../edu_agent/questions/{question_id}/question_metadata.json", 'r', encoding='utf-8') as file:
                grading_rubric = json.load(file).get("grading_rubric")
            if (correct_answer != None) | (grading_rubric != None):
                point_earned, question_focus, get_full_point = self.scoring_fill_in_blank(question_id, student_s_answers["å¡«ç©ºé¢˜"][question_id])
            else:
                point_earned, question_focus, get_full_point, LLM_feedback = self.scoring_by_LLM_without_answer(question_id, student_s_answers["å¡«ç©ºé¢˜"][question_id])
            # ä¿®æ”¹ç°æœ‰é”®çš„å€¼
            answer_revise[question_id] = {"point_earned" : point_earned,
                                        "question_focus" : question_focus,
                                        "get_full_point" : get_full_point,
                                        "LLM_feedback": LLM_feedback
                                        }

        for idx, question_id in enumerate(student_s_answers["è§£ç­”é¢˜"]):
            LLM_feedback = ""
            with open(f"../edu_agent/questions/{question_id}/question_metadata.json", 'r', encoding='utf-8') as file:
                grading_rubric = json.load(file).get("grading_rubric")
            if (correct_answer != None) | (grading_rubric != None):
                point_earned, question_focus, get_full_point, LLM_feedback = self.scoring_comprehensive_problems(question_id, student_s_answers["è§£ç­”é¢˜"][question_id])
            else:
                print(f"ç¬¬{question_id}é¢˜æ²¡æ‰¾åˆ°ç°æˆçš„ç­”æ¡ˆï¼Œè½¬äº¤å¤§æ¨¡å‹æ€è€ƒåˆ¤æ–­ï¼Œè¯·ç¨å")
                point_earned, question_focus, get_full_point, LLM_feedback = self.scoring_by_LLM_without_answer(question_id, student_s_answers["è§£ç­”é¢˜"][question_id])
                print(f"ç¬¬{question_id}é¢˜æ²¡æ‰¾åˆ°ç°æˆçš„ç­”æ¡ˆï¼Œå¤§æ¨¡å‹è‡ªä¸»åˆ¤æ–­å®Œæˆ")
            # ä¿®æ”¹ç°æœ‰é”®çš„å€¼
            answer_revise[question_id] = {"point_earned" : point_earned,
                                        "question_focus" : question_focus,
                                        "get_full_point" : get_full_point,
                                        "LLM_feedback": LLM_feedback
                                        }
    
        # è®¡ç®—æ€»åˆ†
        total_point_earned_of_this_exam = 0
        for item in answer_revise:
            if item != "total_point_earned_of_this_exam":
                #print(answer_revise[item])
                total_point_earned_of_this_exam += answer_revise[item]["point_earned"]
        answer_revise["total_point_earned_of_this_exam"] = total_point_earned_of_this_exam
        

        print(answer_revise)
        # å†™å›æ–‡ä»¶
        with open(student_answer_revise_path, 'w', encoding='utf-8') as file:
            json.dump(answer_revise, file, indent=2, ensure_ascii=False)

    # ========== æ’åä¸è®°å½• ==========
    def generate_student_exam_rank(self):
        def _list_direct_folders(root_path):
            students_paths = []
            path = Path(root_path)
            for item in path.iterdir():
                if item.is_dir():
                    students_paths.append(item.name)
            return students_paths
        students_paths = _list_direct_folders("students")
        student_id_to_total_score = {}
        for item in students_paths:
            student_answer_revise_path = f"../edu_agent/students/{item}/answer_revise.json"
            with open(student_answer_revise_path, 'r', encoding='utf-8') as file:
                answer_revise = json.load(file)
            student_id_to_total_score[item] = answer_revise["total_point_earned_of_this_exam"]
        sorted_student_id_to_total_score = sorted(student_id_to_total_score.items(), key=lambda x: x[1], reverse=True)
        rankings = {}
        current_rank = 1
        draw = 0
        for i, (student_id, score) in enumerate(sorted_student_id_to_total_score):
            if score < sorted_student_id_to_total_score[i-1][1]:
                current_rank = i + draw
                draw = 0
            else: draw += 1    
            rankings[student_id] = current_rank
        return rankings
    
    def write_student_exam_result_into_exam_record(self,student_id_to_rank_dic = {}): 
        def _list_direct_folders(root_path):
            students_paths = []
            path = Path(root_path)
            for item in path.iterdir():
                if item.is_dir():
                    students_paths.append(item.name)
            return students_paths
        students_paths = _list_direct_folders("students")
        for item in students_paths:
            student_answer_revise_path = f"../edu_agent/students/{item}/answer_revise.json"
            student_exam_record_path = f"../edu_agent/students/{item}/exam_record.json"
            with open(student_answer_revise_path, 'r', encoding='utf-8') as file:
                answer_revise = json.load(file)
            with open(student_exam_record_path, 'r', encoding='utf-8') as file:
                exam_record = json.load(file)
            exam_record[self.exam_id] = {}
            exam_record[self.exam_id]["score"] = answer_revise["total_point_earned_of_this_exam"]
            exam_record[self.exam_id]["rank"] = student_id_to_rank_dic[item]
            wrong_focus = []
            for item in answer_revise:
                if item != "total_point_earned_of_this_exam":
                    if answer_revise[item]["get_full_point"] == False:
                        wrong_focus.append(answer_revise[item]["question_focus"])
            exam_record[self.exam_id]["wrong_focus"] = wrong_focus

            with open(student_exam_record_path, 'w', encoding='utf-8') as file:
                json.dump(exam_record, file, indent=2, ensure_ascii=False)

    # ========== å­¦ç”ŸæŠ¥å‘Š ==========
    def generate_report(self,student_dir):
        # è¯»å– json æ–‡ä»¶
        with open(os.path.join(student_dir, "answer_revise.json"), "r", encoding="utf-8") as f:
            answers = json.load(f)
        with open(os.path.join(student_dir, "student_info.json"), "r", encoding="utf-8") as f:
            info = json.load(f)
        with open(os.path.join(student_dir, "exam_record.json"), "r", encoding="utf-8") as f:
            history = json.load(f)

        # å½“å‰è€ƒè¯•ä¿¡æ¯
        current_exam = self.current_exam_number
        current_record = history.get(f"exam_{current_exam}")
        current_rank = current_record["rank"]
        current_score = current_record["score"]

        # é”™è¯¯é¢˜ç›®å’ŒçŸ¥è¯†ç‚¹
        wrong_items = []
        for qid_str, content in answers.items():
            if not isinstance(content, dict):
                continue
            qid = int(qid_str)
            if not content.get("get_full_point", False):
                wrong_items.append((qid, content.get("question_focus", "æ— ")))

        # å†å²åˆ†æ•°ä¸æ’åï¼ˆå»é™¤å½“å‰è¿™æ¬¡å†æ‰‹åŠ¨æ·»åŠ ï¼Œç¡®ä¿é¡ºåºï¼‰
        exam_nums = []
        scores = []
        ranks = []
        for key, record in history.items():
            eid = int(key.replace("exam_", ""))
            if eid != current_exam:
                exam_nums.append(eid)
                scores.append(record["score"])
                ranks.append(record["rank"])
        exam_nums.append(current_exam)
        scores.append(current_score)
        ranks.append(current_rank)
        exam_nums, scores, ranks = zip(*sorted(zip(exam_nums, scores, ranks)))

        # æ–‡æœ¬æ€»ç»“
        rank_progress = ""
        if len(ranks) >= 2:
            delta = ranks[-2] - ranks[-1]
            symbol = "â†‘" if delta > 0 else "â†“"
            rank_progress = f"ä¸ä¸Šæ¬¡ç›¸æ¯”ï¼Œæ’å{symbol}{abs(delta)}åï¼ˆä»ç¬¬{ranks[-2]}ååˆ°ç¬¬{ranks[-1]}åï¼‰"
        if len(ranks) >= 5:
            delta_total = ranks[0] - ranks[-1]
            symbol_total = "â†‘" if delta_total > 0 else "â†“"
            rank_progress += f"ï¼Œä¸ç¬¬ä¸€æ¬¡è€ƒè¯•ç›¸æ¯”{symbol_total}{abs(delta_total)}åï¼ˆä»ç¬¬{ranks[0]}ååˆ°ç¬¬{ranks[-1]}åï¼‰"

        # å›¾è¡¨ç”Ÿæˆ
        fig_dir = os.path.join(student_dir, "report_figures")
        os.makedirs(fig_dir, exist_ok=True)

        # åˆ†æ•°å˜åŒ–
        plt.figure()
        plt.plot(exam_nums, scores, marker="o")
        plt.title("åˆ†æ•°å˜åŒ–æŠ˜çº¿å›¾")
        plt.xlabel("è€ƒè¯•æ¬¡æ•°")
        plt.ylabel("æ€»åˆ†")
        plt.grid(True)
        plt.savefig(os.path.join(fig_dir, "score_trend.png"))
        plt.close()

        # æ’åå˜åŒ–ï¼ˆæ’åå°åœ¨ä¸Šï¼‰
        plt.figure()
        plt.plot(exam_nums, ranks, marker="o")
        plt.gca().invert_yaxis()
        plt.title("æ’åå˜åŒ–æŠ˜çº¿å›¾ï¼ˆè¶Šé«˜è¶Šé ä¸Šï¼‰")
        plt.xlabel("è€ƒè¯•æ¬¡æ•°")
        plt.ylabel("æ’å")
        plt.grid(True)
        plt.savefig(os.path.join(fig_dir, "rank_trend.png"))
        plt.close()

        # Markdown ç”Ÿæˆ
        md_lines = [
            f"# ğŸ“„ å­¦ç”Ÿè€ƒè¯•æŠ¥å‘Šï¼š{info['name']}\n",
            f"## åŸºæœ¬ä¿¡æ¯\n",
            f"- å­¦å·ï¼š{info['student_id']}\n",
            f"- ç­çº§ï¼š{self.fixed_grade} {self.fixed_class}\n",
            f"- æ€§åˆ«ï¼š{info['gender']}\n",
            f"- è€ƒè¯•ç¼–å·ï¼š{self.fixed_paper_id}\n",
            f"- å½“å‰æ€»åˆ†ï¼š{current_score}ï¼Œå½“å‰æ’åï¼šç¬¬{current_rank}å\n",
            f"- {rank_progress}\n",
            "\n## é”™è¯¯é¢˜ç›®ä¸çŸ¥è¯†ç‚¹\n",
        ]
        if wrong_items:
            for qid, focus in wrong_items:
                md_lines.append(f"- é¢˜ç›® {qid}ï¼š{focus}\n")
        else:
            md_lines.append("âœ… æœ¬æ¬¡è€ƒè¯•å…¨éƒ¨ç­”å¯¹ï¼ŒçœŸæ£’ï¼\n")

        md_lines += [
            "\n## å†å²åˆ†æ•°ä¸æ’åå˜åŒ–\n",
            "![åˆ†æ•°å˜åŒ–å›¾](report_figures/score_trend.png)\n",
            "![æ’åå˜åŒ–å›¾](report_figures/rank_trend.png)\n",
        ]

        from openai import OpenAI
        client = OpenAI(
            api_key = "sk-10acd30d8f8c4fbd90b74ed43521b10f", 
            base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        )

        # === LLM æ€»ç»“å»ºè®® ===
        try:
            history_wrong_focuses = []
            for exam in history.values():
                focus = exam.get("wrong_focus")
                if isinstance(focus, list):
                    history_wrong_focuses.extend(focus)
                elif isinstance(focus, str) and focus.strip():
                    history_wrong_focuses.append(focus)
            history_focus_text = "ï¼Œ".join(map(str, history_wrong_focuses)) if history_wrong_focuses else "æ— "
            current_wrong_focus_text = "ï¼Œ".join([f"{qid}-{focus}" for qid, focus in wrong_items]) if wrong_items else "æ— "

            prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸­å­¦ç”Ÿå­¦ä¸šè¾…å¯¼ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼Œä¸ºè¯¥å­¦ç”Ÿæ’°å†™ä¸€æ®µä¸è¶…è¿‡150å­—çš„è€ƒè¯•åé¦ˆå»ºè®®ï¼ŒæŒ‡å‡ºå…¶è¿›æ­¥ä¸ä¸è¶³ï¼Œå¹¶ç»™äºˆé¼“åŠ±å’Œæ”¹è¿›å»ºè®®ã€‚

    å­¦ç”Ÿå§“åï¼š{info['name']}
    è¿‡å¾€åˆ†æ•°ï¼š{list(scores[:-1])}
    å½“å‰åˆ†æ•°ï¼š{current_score}
    è¿‡å¾€æ’åï¼š{list(ranks[:-1])}
    å½“å‰æ’åï¼š{current_rank}
    å†å²é”™è¯¯çŸ¥è¯†ç‚¹ï¼š{history_focus_text}
    æœ¬æ¬¡é”™è¯¯é¢˜ç›®ä¸çŸ¥è¯†ç‚¹ï¼š{current_wrong_focus_text}
    è¯·è¾“å‡ºç®€æ´å‡ç»ƒçš„åé¦ˆå»ºè®®ï¼š
    """

            response = client.chat.completions.create(
                model='qwen3-32b', 
                messages=[{'role': 'user', 'content': prompt}],
                extra_body = {"enable_thinking": False}
            )
            advice_text = response.choices[0].message.content
            md_lines += ["\n## ğŸ’¬ å­¦ä¹ å»ºè®®ï¼ˆç”± AI ç”Ÿæˆï¼‰\n", f"{advice_text}\n"]

        except Exception as e:
            print(f"âš ï¸ AI ç”Ÿæˆå»ºè®®å¤±è´¥ï¼š{e}")

        with open(os.path.join(student_dir, "student_report.md"), "w", encoding="utf-8") as f:
            f.writelines(md_lines)

        print(f"âœ… å·²ç”ŸæˆæŠ¥å‘Šï¼š{student_dir}/student_report.md")

    # ========== æ•™å¸ˆæŠ¥å‘Š ==========
    def parse_all_student_data(self,students_root="students"):
        all_student_records = []
        for sid in os.listdir(students_root):
            spath = os.path.join(students_root, sid)
            if not os.path.isdir(spath):
                continue
            try:
                with open(os.path.join(spath, "student_info.json"), "r", encoding="utf-8") as f:
                    info = json.load(f)
                with open(os.path.join(spath, "answer_revise.json"), "r", encoding="utf-8") as f:
                    ans = json.load(f)
                with open(os.path.join(spath, "exam_record.json"), "r", encoding="utf-8") as f:
                    hist = json.load(f)

                current_key = f"exam_{self.current_exam_number}"
                current_record = hist.get(current_key)
                current_rank = current_record["rank"]
                current_score = current_record["score"]


                all_student_records.append({
                    "student_id": info["student_id"],
                    "name": info["name"],
                    "gender": info["gender"],
                    "grade": self.grade,
                    "class": self.class_id,
                    "paper_id": self.paper_id,
                    "exam_number": self.current_exam_number,
                    "score": current_score,
                    "rank": current_rank,
                    "history": copy.deepcopy(hist),
                    "answers": ans,
                })

            except Exception as e:
                print(f"è·³è¿‡ {sid}ï¼š{e}")
        return all_student_records

    def generate_teacher_report(self,all_student_records):
        os.makedirs("teacher_figures", exist_ok=True)

        # ä¸€ã€åˆ†æ•°åˆ†å¸ƒ
        scores = [s["score"] for s in all_student_records]
        plt.figure(figsize=(8, 4))
        sns.histplot(scores, bins=10, kde=True)
        plt.xlabel("åˆ†æ•°")
        plt.ylabel("å­¦ç”Ÿæ•°é‡")
        plt.title("å­¦ç”Ÿåˆ†æ•°é¢‘ç‡åˆ†å¸ƒ")
        plt.savefig("teacher_figures/score_distribution_histogram.png")
        plt.close()

        # äºŒã€åˆ†æ•°è¶‹åŠ¿
        exam_stats = defaultdict(lambda: {"score": []})
        for stu in all_student_records:
            for k, v in stu["history"].items():
                exam_num = int(k.split("_")[-1])
                exam_stats[exam_num]["score"].append(v["score"])

        stats_list = []
        for exam_num in sorted(exam_stats.keys()):
            scores = exam_stats[exam_num]["score"]
            stats_list.append({
                "exam": f"Exam {exam_num}",
                "mean": sum(scores) / len(scores),
                "max": max(scores),
                "min": min(scores),
                "median": median(scores)
            })

        df_stats = pd.DataFrame(stats_list)
        plt.figure(figsize=(8, 4))
        for col in ["mean", "max", "min", "median"]:
            plt.plot(df_stats["exam"], df_stats[col], marker="o", label=col)
        plt.ylabel("åˆ†æ•°")
        plt.title("è€ƒè¯•æˆç»©ç»Ÿè®¡è¶‹åŠ¿")
        plt.legend()
        plt.savefig("teacher_figures/score_statistics_trend.png")
        plt.close()

        # ä¸‰ã€é¢˜ç›® & çŸ¥è¯†ç‚¹ æ­£ç¡®ç‡ï¼ˆä»…çœ‹æ˜¯å¦æ»¡åˆ†ï¼‰
        question_correct = defaultdict(list)
        knowledge_correct = defaultdict(lambda: {"correct": 0, "total": 0})

        for stu in all_student_records:
            for qid, ans in stu["answers"].items():
                if not isinstance(ans, dict):
                    continue
                is_correct = ans.get("get_full_point", False)
                question_correct[qid].append(is_correct)

                focus = ans.get("question_focus", "æœªçŸ¥çŸ¥è¯†ç‚¹")
                knowledge_correct[focus]["total"] += 1
                if is_correct:
                    knowledge_correct[focus]["correct"] += 1

        # é¢˜ç›®æ­£ç¡®ç‡å›¾è¡¨
        question_accuracy_df = pd.DataFrame([{
            "question": qid,
            "accuracy": sum(v) / len(v)
        } for qid, v in question_correct.items()])
        question_accuracy_df = question_accuracy_df.sort_values("question", key=lambda x: x.astype(int))

        min_5 = question_accuracy_df.nsmallest(5, "accuracy")["question"].tolist()
        bar_colors = ["red" if q in min_5 else "blue" for q in question_accuracy_df["question"]]

        plt.figure(figsize=(10, 4))
        sns.barplot(x="question", y="accuracy", data=question_accuracy_df, palette=bar_colors)
        plt.title("æ¯é“é¢˜æ­£ç¡®ç‡")
        plt.savefig("teacher_figures/question_accuracy_bar.png")
        plt.close()

        # çŸ¥è¯†ç‚¹æ­£ç¡®ç‡å›¾è¡¨
        knowledge_df = pd.DataFrame([{
            "knowledge": k,
            "accuracy": v["correct"] / v["total"] if v["total"] > 0 else 0
        } for k, v in knowledge_correct.items()])
        knowledge_df = knowledge_df.sort_values("accuracy")

        plt.figure(figsize=(10, 4))
        sns.barplot(x="knowledge", y="accuracy", data=knowledge_df, palette="Blues_d")
        plt.xticks(rotation=45, ha="right")
        plt.title("çŸ¥è¯†ç‚¹æ­£ç¡®ç‡")
        plt.tight_layout()
        plt.savefig("teacher_figures/knowledge_accuracy_bar.png")
        plt.close()

        # å››ã€å‰äº”å / è¿›æ­¥ / é€€æ­¥
        top5 = sorted(all_student_records, key=lambda x: x["rank"])[:5]
        top5_names = [s["name"] for s in top5]

        for stu in all_student_records:
            history = stu["history"]

            # è·å–æ‰€æœ‰ exam_x ä¸­çš„ exam_number å’Œ rankï¼ˆåŒ…æ‹¬ exam_5ï¼‰
            ranks = sorted(
                [(int(k.split("_")[-1]), v["rank"]) for k, v in history.items()]
            )

            # åˆ†ææ’åå˜åŒ–ï¼šrank è¶Šå°è¶Šå¥½ï¼ˆåæ¬¡ä¸Šå‡ï¼‰
            stu["rank_diff"] = ranks[0][1] - ranks[-1][1] if len(ranks) >= 2 else 0
            stu["rank_range"] = f"{ranks[0][1]} â†’ {ranks[-1][1]}" if len(ranks) >= 2 else "æ— "

        df = pd.DataFrame(all_student_records)
        progress_top5 = df.sort_values("rank_diff", ascending=False).head()
        decline_top5 = df.sort_values("rank_diff").head()

        # äº”ã€å¹³å‡æ’åè¡¨
        exam_range = range(1, 6)
        avg_rank_df = pd.DataFrame(index=[s["name"] for s in all_student_records])
        for e in exam_range:
            avg_rank_df[f"Exam {e}"] = [
                s["history"].get(f"exam_{e}", {}).get("rank", None) if s["exam_number"] != e else s["rank"]
                for s in all_student_records
            ]
        avg_rank_df["å¹³å‡æ’å"] = avg_rank_df.mean(axis=1)
        avg_rank_df_sorted = avg_rank_df.sort_values("å¹³å‡æ’å")
        avg_rank_df_sorted.to_excel("teacher_figures/äº”æ¬¡è€ƒè¯•å¹³å‡æ’åè¡¨.xlsx")

        # å…­ã€Markdown æŠ¥å‘Šå†™å…¥
        today = datetime.date.today().strftime("%Y-%m-%d")
        md = f"""# ğŸ“˜ æ•™å¸ˆæŠ¥å‘Šæ±‡æ€»ï¼ˆ{today}ï¼‰

    ## å¹´çº§ç­çº§ä¿¡æ¯
    - å¹´çº§ï¼š{self.grade}  ç­çº§ï¼š{self.class_id}
    - è€ƒè¯•ç¼–å·ï¼š{self.paper_id}  è€ƒè¯•æ¬¡æ•°ï¼šExam {self.current_exam_number}

    ## ä¸€ã€å­¦ç”Ÿåˆ†æ•°é¢‘ç‡åˆ†å¸ƒç›´æ–¹å›¾
    ![åˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾](/teacher_figures/score_distribution_histogram.png)

    ## äºŒã€åˆ†æ•°ç»Ÿè®¡å˜åŒ–è¶‹åŠ¿ï¼ˆäº”æ¬¡è€ƒè¯•ï¼‰
    åŒ…æ‹¬å¹³å‡åˆ†ã€æœ€é«˜åˆ†ã€æœ€ä½åˆ†ã€ä¸­ä½æ•°éšè€ƒè¯•æ¬¡æ•°çš„å˜åŒ–ï¼š
    ![ç»Ÿè®¡è¶‹åŠ¿å›¾](/teacher_figures/score_statistics_trend.png)

    ## ä¸‰ã€æ¯é“é¢˜çš„æ­£ç¡®ç‡
    å¾—åˆ†ç‡æœ€ä½çš„äº”é¢˜å·²ç”¨çº¢è‰²æŸ±æ ‡å‡ºï¼š
    ![é¢˜ç›®å¾—åˆ†ç‡](/teacher_figures/question_accuracy_bar.png)

    ## å››ã€æ¯ä¸ªçŸ¥è¯†ç‚¹çš„æ­£ç¡®ç‡
    ![çŸ¥è¯†ç‚¹å¾—åˆ†ç‡](/teacher_figures/knowledge_accuracy_bar.png)

    ## äº”ã€è€ƒè¯•è¡¨ç°æ€»ç»“

    ### ğŸ† æœ€è¿‘ä¸€æ¬¡è€ƒè¯•å‰äº”åï¼š
    {", ".join(top5_names)}

    ### ğŸ“ˆ è¿›æ­¥å‰äº”åï¼š
    """ + "\n".join([f"- {idx+1}. {row['name']}ï¼šâ†‘{row['rank_diff']}åï¼ˆ{row['rank_range']}ï¼‰" for idx, row in progress_top5.iterrows()]) + """

    ### ğŸ“‰ é€€æ­¥å‰äº”åï¼š
    """ + "\n".join([f"- {idx+1}. {row['name']}ï¼šâ†“{-row['rank_diff']}åï¼ˆ{row['rank_range']}ï¼‰" for idx, row in decline_top5.iterrows()]) + """

    ## å…­ã€äº”æ¬¡è€ƒè¯•å¹³å‡æ’åè¡¨
    - æ–‡ä»¶ï¼š`teacher_figures/äº”æ¬¡è€ƒè¯•å¹³å‡æ’åè¡¨.xlsx`
    """

        # ä¸ƒã€AI æ•™å­¦å»ºè®®
        from openai import OpenAI
        client = OpenAI(
            api_key = "sk-10acd30d8f8c4fbd90b74ed43521b10f", 
            base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        try:
            worst_q = question_accuracy_df.nsmallest(5, "accuracy").values.tolist()
            worst_focus = knowledge_df.nsmallest(5, "accuracy").values.tolist()

            prompt = f"""ä½ æ˜¯ä¸€ä½ä¸­å­¦æ•™å¸ˆï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ•°æ®æ’°å†™æ•™å­¦å»ºè®®ï¼š

    - æ­£ç¡®ç‡æœ€ä½çš„é¢˜ç›®ï¼š{', '.join([f"ç¬¬{int(q[0])}é¢˜ï¼ˆ{q[1]*100:.1f}%ï¼‰" for q in worst_q])}
    - æ­£ç¡®ç‡æœ€ä½çš„çŸ¥è¯†ç‚¹ï¼š{', '.join([f"{k[0]}ï¼ˆ{k[1]*100:.1f}%ï¼‰" for k in worst_focus])}
    - è¿›æ­¥å‰äº”ï¼š{', '.join([f"{s['name']}ï¼ˆ+{s['rank_diff']}åï¼‰" for s in progress_top5.to_dict(orient='records')])}
    - é€€æ­¥å‰äº”ï¼š{', '.join([f"{s['name']}ï¼ˆ{s['rank_diff']}åï¼‰" for s in decline_top5.to_dict(orient='records')])}

    è¯·ï¼š
    1. æŒ‡å‡ºé‡ç‚¹è®²è§£çš„é¢˜ç›®/çŸ¥è¯†ç‚¹ï¼›
    2. é’ˆå¯¹å­¦ç”Ÿè¡¨ç°æå‡ºå»ºè®®ï¼›
    3. ç»™è¿›æ­¥å­¦ç”Ÿé¼“åŠ±ï¼›
    4. ç»™é€€æ­¥å­¦ç”Ÿæä¾›å…³æ³¨å»ºè®®ã€‚
    """



            response = client.chat.completions.create(
                model='qwen3-32b',  
                messages=[{'role': 'user', 'content': prompt}],
                extra_body = {"enable_thinking": False}
            )
            advice_text = response.choices[0].message.content
            md += "\n## ä¸ƒã€ğŸ’¡ AI æ•™å­¦å»ºè®®\n" + advice_text + "\n"
        except Exception as e:
            print(f"âš ï¸ æ•™å­¦å»ºè®®ç”Ÿæˆå¤±è´¥ï¼š{e}")
            md += "\n## ä¸ƒã€ğŸ’¡ AI æ•™å­¦å»ºè®®\nç”Ÿæˆå¤±è´¥ã€‚\n"

        with open("teacher_report.md", "w", encoding="utf-8") as f:
            f.write(md)

        print("âœ… æ•™å¸ˆæŠ¥å‘Šç”ŸæˆæˆåŠŸï¼")

# ========== ä½¿ç”¨ç¤ºä¾‹ ==========
# if __name__ == "__main__":
#     manager = EduAgent()
#     print("ğŸ¤–æ­£åœ¨æå–è¯•å·ç­”æ¡ˆ")
#     manager.student_s_answers_extractor(student_id=2024001)
#     print("âœ…æˆåŠŸæå–ç¬¬ä¸€ä¸ªå­¦ç”Ÿçš„è¯•å·ç­”æ¡ˆ")
#     manager.scoring_executor(student_id=2024001)
#     print("âœ…æˆåŠŸæ‰¹æ”¹ç¬¬ä¸€ä¸ªå­¦ç”Ÿçš„è¯•å·ç­”æ¡ˆ")
#     student_id_to_rank_dic = manager.generate_student_exam_rank()
#     print("âœ…æˆåŠŸäº§ç”Ÿæœ¬æ¬¡è€ƒè¯•çš„æ’å")
#     manager.write_student_exam_result_into_exam_record(student_id_to_rank_dic)
#     print("âœ…å½•å…¥æœ¬æ¬¡è€ƒè¯•æ¯ä¸ªå­¦ç”Ÿçš„è€ƒè¯•è®°å½•")
#     # æ‰¹é‡ç”Ÿæˆå­¦ç”ŸæŠ¥å‘Š
#     for sid in os.listdir(manager.students_root):
#         student_path = os.path.join(manager.students_root, sid)
#         if os.path.isdir(student_path):
#             try:
#                 manager.generate_report(student_path)
#             except Exception as e:
#                 print(f"âš ï¸ è·³è¿‡ {sid}ï¼š{e}")
#     # ç”Ÿæˆæ•™å¸ˆæŠ¥å‘Š
#     records = manager.parse_all_student_data()
#     manager.generate_teacher_report(records)
