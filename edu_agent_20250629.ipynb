{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EduAgent20250629"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è·‘é€šæ‰¹æ”¹æµç¨‹ï¼ŒéªŒè¯äº†å¤§é¢˜é’ˆå¯¹æ€§æ‰¹æ”¹ç­–ç•¥çš„å¯è¡Œæ€§\n",
    "## æ”¿æ²»ã€ç‰©ç†ã€æ•°å­¦ã€åœ°ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ€è·¯ï¼šå…ˆç”¨APIè¯·æ±‚æœ€å¼ºå¤§çš„æ¨¡å‹éªŒè¯å¯è¡Œæ€§ï¼›å†è¿ç§»åˆ°å¤‡é€‰çš„å¼€æºæ¨¡å‹ï¼›æœ€ç»ˆè½å®åˆ°æœ¬åœ°éƒ¨ç½²æ¨¡å‹æ¥ç®¡å…¨æµç¨‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç‰©ç† æ•°å­¦ï¼šæŠŠç­”æ¡ˆæ‹†åˆ†ä¸ºåŸºæœ¬çš„ç»™åˆ†å•å…ƒï¼Œå¯¹æ¯ä¸ªç»™åˆ†å•å…ƒè¿›è¡Œè¯¦ç»†è¯„åˆ†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![å›¾ç‰‡æè¿°](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Literal\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "question_path = \"question.txt\"\n",
    "ground_truth_answer_path = \"ground_truth_answer.txt\"\n",
    "explanation_of_ground_truth_answer_path = \"explanation.txt\"\n",
    "few_shot_path = \"few_shot_example.json\"\n",
    "student_answer_path = \"student_answer.txt\"\n",
    "\n",
    "question = pd.read_table(f\"{question_path}\")\n",
    "ground_truth_answer = pd.read_table(f\"{ground_truth_answer_path}\")\n",
    "explanation_of_ground_truth_answer = pd.read_table(f\"{explanation_of_ground_truth_answer_path}\")\n",
    "few_shot = pd.read_table(f\"{few_shot_path}\")\n",
    "student_answer = pd.read_table(f\"{student_answer_path}\")\n",
    "\n",
    "class PointsEarnedAndWhy(BaseModel):\n",
    "    points_earned_of_this_equation:int\n",
    "    why: str\n",
    "\n",
    "class CorrectionAndExplanation(BaseModel):\n",
    "    formula1 : PointsEarnedAndWhy\n",
    "    formula2 : PointsEarnedAndWhy\n",
    "    formula3 : PointsEarnedAndWhy\n",
    "\n",
    "# å®šä¹‰ç»“æ„åŒ–è¾“å‡ºçš„æ•°æ®æ¨¡å‹\n",
    "class QuestionGrading(BaseModel):\n",
    "    points_earned_of_this_question: float\n",
    "    correction_and_explanation: CorrectionAndExplanation\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"o4-mini\",\n",
    "    input=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"input_text\", \"text\": \"ç°åœ¨ä½ æ˜¯ä¸€ä¸ªä¸­å­¦è€å¸ˆï¼Œä½ è¦è´Ÿè´£æ‰¹æ”¹ä½ å­¦ç”Ÿçš„ç‰©ç†è¯•å·çš„é¢˜ç›®ï¼Œä¸»ä»»è¦æ±‚ä½ ä¸¥æ ¼æŒ‰ç…§é¢˜ç›®è§£æä¸å„ç‚¹è¯„åˆ†æ ‡å‡†é‡Œçš„è¸©åˆ†ç‚¹æ¥è¿›è¡Œæ‰¹æ”¹å¾—åˆ†ï¼Œå¹¶å¯¹æ¯ä¸€ä¸ªè¸©åˆ†ç‚¹è¿›è¡Œè§£é‡Šï¼Œæ¯”å¦‚è¯´è¿™ä¸ªè¸©åˆ†ç‚¹æœ‰å¯¹åº”çš„å…¬å¼ï¼Œå¾—åˆ°ç›¸åº”çš„åˆ†æ•°ï¼Œé‚£ä¸ªè¸©åˆ†ç‚¹æ²¡æœ‰å…¬å¼æˆ–è€…å…¬å¼é”™è¯¯ï¼Œä¸å¾—åˆ†\"}, # è§’è‰²æ‰®æ¼” èƒŒæ™¯ä»‹ç»\n",
    "            {\"type\": \"input_text\", \"text\": f\"é¢˜ç›®ï¼š{question}\"},\n",
    "            {\"type\": \"input_text\", \"text\": f\"æ ‡å‡†ç­”æ¡ˆï¼š{ground_truth_answer}\"},\n",
    "            {\"type\": \"input_text\", \"text\": f\"é¢˜ç›®è§£æä¸å„ç‚¹è¯„åˆ†æ ‡å‡†ï¼š{explanation_of_ground_truth_answer}\"},\n",
    "            {\"type\": \"input_text\", \"text\": f\"æ‰¹æ”¹ç¤ºä¾‹ï¼š{few_shot}\"},\n",
    "            {\"type\": \"input_text\", \"text\": f\"å­¦ç”Ÿç­”æ¡ˆï¼š{student_answer}\"},\n",
    "        ],\n",
    "    }],\n",
    "    text_format = QuestionGrading,\n",
    ")\n",
    "\n",
    "# print(\"ç°åœ¨ä½ æ˜¯ä¸€ä¸ªä¸­å­¦è€å¸ˆï¼Œä½ è¦è´Ÿè´£æ‰¹æ”¹ä½ å­¦ç”Ÿçš„ç‰©ç†è¯•å·çš„é¢˜ç›®ï¼Œä¸»ä»»è¦æ±‚ä½ ä¸¥æ ¼æŒ‰ç…§é¢˜ç›®è§£æä¸å„ç‚¹è¯„åˆ†æ ‡å‡†é‡Œçš„è¸©åˆ†ç‚¹æ¥è¿›è¡Œæ‰¹æ”¹å¾—åˆ†ï¼Œå¹¶å¯¹æ¯ä¸€ä¸ªè¸©åˆ†ç‚¹è¿›è¡Œè§£é‡Šï¼Œæ¯”å¦‚è¯´è¿™ä¸ªè¸©åˆ†ç‚¹æœ‰å¯¹åº”çš„å…¬å¼ï¼Œå¾—åˆ°ç›¸åº”çš„åˆ†æ•°ï¼Œé‚£ä¸ªè¸©åˆ†ç‚¹æ²¡æœ‰å…¬å¼æˆ–è€…å…¬å¼é”™è¯¯ï¼Œä¸å¾—åˆ†\")\n",
    "# print(f\"é¢˜ç›®ï¼š{question}\")\n",
    "# print(f\"æ ‡å‡†ç­”æ¡ˆï¼š{ground_truth_answer}\")\n",
    "# print(f\"é¢˜ç›®è§£æä¸å„ç‚¹è¯„åˆ†æ ‡å‡†ï¼š{explanation_of_ground_truth_answer}\")\n",
    "# print(f\"æ‰¹æ”¹ç¤ºä¾‹ï¼š{few_shot}\")\n",
    "# print(f\"å­¦ç”Ÿç­”æ¡ˆï¼š{student_answer}\")\n",
    "print(\"ç¬¬ä¸€è½®å¯¹è¯\",response.output_parsed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ”¿æ²» å†å² åœ°ç†ï¼šé«˜äº®æ ‡è¯†å¯èƒ½çš„å¾—åˆ†ç‚¹ï¼Œå¤§å¤§å‡è½»é˜…å·è€å¸ˆçš„è§†è§‰è´Ÿæ‹…ï¼ŒæŠŠå¤æ‚çš„ç»¼åˆç ”åˆ¤è¿‡ç¨‹ç®€åŒ–ä¸ºâ€œæ¥å—æˆ–æ‹’ç»â€œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ”¿æ²»ï¼šæ•™æåŸæ–‡ä¼˜å…ˆï¼Œç”¨ç¡®å®šæ€§æŠ€æœ¯æ–¹æ¡ˆæ¥åŒ¹é… --> difflib æœ€å¤§å…¬å…±å­—ç¬¦ä¸²\n",
    "### å†å² åœ°ç†ï¼šè¯­ä¹‰ç©ºé—´æ›´åŠ çµæ´»ï¼Œå¯é‡‡ç”¨è‡ªç„¶è¯­ä¹‰åŒ¹é… --> æœ¬åœ°éƒ¨ç½²embeddingæ¨¡å‹ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCDEFGç»æµå…¨çƒåŒ–: 0.5882352941176471\n",
      "ç»æµå…¨çƒ: 0.8888888888888888\n",
      "ç»æµå…¨çƒåŒ–: 1.0\n",
      "ç»æµçš„å…¨çƒåŒ–: 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def get_similarity(target_str, compare_str):\n",
    "    return SequenceMatcher(None, target_str, compare_str).ratio()\n",
    "\n",
    "target_str = \"ç»æµå…¨çƒåŒ–\"\n",
    "compare_set = [\"ABCDEFGç»æµå…¨çƒåŒ–\", \"ç»æµå…¨çƒ\", \"ç»æµå…¨çƒåŒ–\", \"ç»æµçš„å…¨çƒåŒ–\"]\n",
    "\n",
    "for compare_str in compare_set:\n",
    "    similarity = get_similarity(target_str, compare_str)\n",
    "    print(f\"{compare_str}: {similarity}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from paddleocr import PaddleOCR\n",
    "import ollama\n",
    "import chromadb\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def get_center_y(box):\n",
    "    return (box[0][1] + box[3][1]) / 2\n",
    "\n",
    "def group_by_lines(char_list, y_thresh=15):\n",
    "    \"\"\"å°†å­—ç¬¦æŒ‰Yåæ ‡èšç±»ä¸ºå¤šè¡Œ\"\"\"\n",
    "    lines = []\n",
    "    for ch in char_list:\n",
    "        cy = get_center_y(ch['box'])\n",
    "        matched = False\n",
    "        for line in lines:\n",
    "            line_cy = get_center_y(line[0]['box'])\n",
    "            if abs(cy - line_cy) < y_thresh:\n",
    "                line.append(ch)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            lines.append([ch])\n",
    "    return lines\n",
    "\n",
    "def build_colored_path_from_lines(line_groups):\n",
    "    \"\"\"å°†å¤šè¡Œå­—ç¬¦æ„é€ æˆä¸€ä¸ªè¿ç»­æŸ“è‰²åŒºåŸŸï¼ˆé¦–å°¾å­—ç¬¦æ‹¼æ¥ï¼‰\"\"\"\n",
    "    path_points = []\n",
    "    for line in line_groups:\n",
    "        first = line[0]['box']\n",
    "        last = line[-1]['box']\n",
    "        tl = first[0]\n",
    "        bl = first[3]\n",
    "        tr = last[1]\n",
    "        br = last[2]\n",
    "        path_points.extend([tl, tr, br, bl])\n",
    "    return [list(map(int, pt)) for pt in path_points]\n",
    "\n",
    "# åˆå§‹åŒ– OCR\n",
    "ocr = PaddleOCR(use_textline_orientation=True, lang='ch')\n",
    "res = ocr.predict(\"image_input.png\")[0]\n",
    "\n",
    "texts = res['rec_texts']\n",
    "scores = res['rec_scores']\n",
    "boxes = res['rec_polys']\n",
    "\n",
    "# æ„å»ºå­—ç¬¦çº§åºåˆ—\n",
    "char_stream = []\n",
    "for text, score, box in zip(texts, scores, boxes):\n",
    "    chars = list(text.strip())\n",
    "    n = len(chars)\n",
    "    box = np.array(box)\n",
    "    top_line = np.linspace(box[0], box[1], n + 1)\n",
    "    bottom_line = np.linspace(box[3], box[2], n + 1)\n",
    "    for i, ch in enumerate(chars):\n",
    "        tl = top_line[i]\n",
    "        tr = top_line[i+1]\n",
    "        br = bottom_line[i+1]\n",
    "        bl = bottom_line[i]\n",
    "        char_box = np.array([tl, tr, br, bl], dtype=int).tolist()\n",
    "        char_stream.append({'char': ch, 'box': char_box, 'score': score})\n",
    "\n",
    "print(char_stream[0])\n",
    "\n",
    "# æ„å»ºå…¨æ–‡\n",
    "full_text = ''.join([c['char'] for c in char_stream])\n",
    "\n",
    "# åˆ‡åˆ†ä¸ºå¥å­ï¼ˆæ’é™¤é¡¿å·ï¼‰\n",
    "split_pattern = re.compile(r\"([ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š])\")\n",
    "segments = []\n",
    "start_idx = 0\n",
    "for match in split_pattern.finditer(full_text):\n",
    "    end_idx = match.end()\n",
    "    segment_text = full_text[start_idx:end_idx]\n",
    "    segment_chars = char_stream[start_idx:end_idx]\n",
    "    line_groups = group_by_lines(segment_chars)\n",
    "    polygon_path = build_colored_path_from_lines(line_groups)\n",
    "    segments.append({\n",
    "        'text': segment_text,\n",
    "        'box': polygon_path,\n",
    "        'score': np.mean([c['score'] for c in segment_chars])\n",
    "    })\n",
    "    start_idx = end_idx\n",
    "\n",
    "# å¤„ç†ç»“å°¾æ®‹ä½™\n",
    "if start_idx < len(full_text):\n",
    "    segment_chars = char_stream[start_idx:]\n",
    "    segment_text = full_text[start_idx:]\n",
    "    line_groups = group_by_lines(segment_chars)\n",
    "    polygon_path = build_colored_path_from_lines(line_groups)\n",
    "    segments.append({\n",
    "        'text': segment_text,\n",
    "        'box': polygon_path,\n",
    "        'score': np.mean([c['score'] for c in segment_chars])\n",
    "    })\n",
    "'''\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"test_2\")\n",
    "\n",
    "# store each document in a vector embedding database\n",
    "for i, item in enumerate(segments):\n",
    "  response = ollama.embed(model=\"mxbai-embed-large\", input=item['text'])\n",
    "  embeddings = response[\"embeddings\"]\n",
    "  collection.add(\n",
    "    ids=[str(i)],\n",
    "    embeddings=embeddings,\n",
    "    documents=[item['text']],\n",
    "  )\n",
    "\n",
    "target = \"æ¨åŠ¨ç»æµå…¨çƒåŒ–æœç€æ›´åŠ å¼€æ”¾ã€åŒ…å®¹ã€æ™®æƒ ã€å¹³è¡¡çš„æ–¹å‘å‘å±•\"\n",
    "target_embedding = ollama.embed(\n",
    "  model=\"mxbai-embed-large\",\n",
    "  input=target\n",
    ")\n",
    "query_result = collection.query(\n",
    "  query_embeddings=[target_embedding[\"embeddings\"][0]],\n",
    "  n_results=1\n",
    ")\n",
    "best_match = segments[int(query_result['ids'][0][0])]\n",
    "'''\n",
    "\n",
    "#difflib åŒ¹é…å…³é”®è¯\n",
    "target = \"æ¨åŠ¨ç»æµå…¨çƒåŒ–æœç€æ›´åŠ å¼€æ”¾ã€åŒ…å®¹ã€æ™®æƒ ã€å¹³è¡¡çš„æ–¹å‘å‘å±•\"\n",
    "best_match = None\n",
    "best_score = 0\n",
    "\n",
    "for item in segments:\n",
    "    sim = SequenceMatcher(None, item['text'], target).ratio()\n",
    "    item['sim'] = sim\n",
    "    if sim > best_score:\n",
    "        best_score = sim\n",
    "        best_match = item\n",
    "\n",
    "# è¾“å‡ºç»“æœ\n",
    "print(\"ğŸ¯ åŒ¹é…ç»“æœï¼ˆæŒ‰é‡ç»„åå¥å­ï¼‰:\")\n",
    "for item in segments:\n",
    "    print(f\"æ–‡å­—ï¼š{item['text']}ï¼Œç›¸ä¼¼åº¦ï¼š{item['sim']:.2f}ï¼Œåæ ‡ç‚¹æ•°ï¼š{len(item['box'])}ï¼Œåæ ‡ï¼š{item['box']}\")\n",
    "\n",
    "print(\"\\nğŸ”¥ æœ€ä½³åŒ¹é…:\")\n",
    "print(f\"æ–‡æœ¬ï¼š{best_match['text']}ï¼Œç›¸ä¼¼åº¦ï¼š{best_match['sim']:.2f}ï¼Œåæ ‡ï¼š{best_match['box']}\")\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# åŠ è½½åŸå›¾ï¼ˆç¡®ä¿æ˜¯RGBAï¼‰\n",
    "base = Image.open(\"image_input.png\").convert(\"RGBA\")\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªé€æ˜å›¾å±‚\n",
    "overlay = Image.new(\"RGBA\", base.size, (0, 0, 0, 0))\n",
    "draw = ImageDraw.Draw(overlay)\n",
    "\n",
    "# ç»˜åˆ¶åŠé€æ˜çº¢è‰²å¤šè¾¹å½¢åˆ° overlayä¸Š\n",
    "i = 0\n",
    "\n",
    "while i < len(best_match['box']):\n",
    "    draw.polygon(best_match['box'][i:i+4], fill=(255, 0, 0, 100))  # alpha=100 è¡¨ç¤ºåŠé€æ˜\n",
    "    i += 4\n",
    "\n",
    "# åˆæˆåŸå›¾ä¸æŸ“è‰²å›¾å±‚\n",
    "out = Image.alpha_composite(base, overlay)\n",
    "\n",
    "# ä¿å­˜æœ€ç»ˆæ•ˆæœ\n",
    "out.save(\"output_baidu.png\")\n",
    "print(\"âœ… æŸ“è‰²ç»“æœå·²æ›´æ–°ä¸ºåŠé€æ˜çº¢è‰²å¹¶ä¿å­˜ä¸º output_baidu.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æµ‹è¯•qwenç³»åˆ—èƒ½åŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'points_earned_of_this_question': 4, 'correction_and_explanation': {'r_{ä¹™} = \\\\frac{L}{\\\\sin 30^\\\\circ} = 2L': {'è¯¥ç‚¹å¾—åˆ†': 2, 'åŸå› ': 'å­˜åœ¨è¯¥å…¬å¼æˆ–å…¶ç›¸åŒæ•°å­¦é€»è¾‘çš„å½¢å¼ï¼Œå¾—2åˆ†'}, 'q v_0 B = m \\\\frac{v_0^2}{r_ä¹™}': {'è¯¥ç‚¹å¾—åˆ†': 2, 'åŸå› ': 'å­˜åœ¨è¯¥å…¬å¼æˆ–å…¶ç›¸åŒæ•°å­¦é€»è¾‘çš„å½¢å¼ï¼Œå¾—2åˆ†'}, 'B = \\\\frac{m v_0}{2 q L}': {'è¯¥ç‚¹å¾—åˆ†': 0, 'åŸå› ': 'æœ€ç»ˆç»“æœè®¡ç®—é”™è¯¯ï¼Œæ­£ç¡®è¡¨è¾¾å¼åº”ä¸º $ B = \\\\frac{m v_0}{2 q L} $ï¼Œè€Œå­¦ç”Ÿç­”æ¡ˆç¼ºå°‘äº†åˆ†æ¯ä¸­çš„ $ L $ï¼Œå› æ­¤ä¸å¾—åˆ†'}}}\n"
     ]
    }
   ],
   "source": [
    "# æ­¥éª¤ 1ï¼šå‘å‡ºè¯·æ±‚\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "question_path = \"question.txt\"\n",
    "ground_truth_answer_path = \"ground_truth_answer.txt\"\n",
    "explanation_of_ground_truth_answer_path = \"explanation.txt\"\n",
    "few_shot_path = \"few_shot.json\"\n",
    "student_answer_path = \"student_answer.txt\"\n",
    "\n",
    "with open(f'{question_path}', 'r', encoding='utf-8') as question_file,\\\n",
    "    open(f'{ground_truth_answer_path}', 'r', encoding='utf-8') as ground_truth_answer_file,\\\n",
    "    open(f'{explanation_of_ground_truth_answer_path}', 'r', encoding='utf-8') as explanation_of_ground_truth_answer_file,\\\n",
    "    open(f'{few_shot_path}', 'r', encoding='utf-8') as few_shot_file,\\\n",
    "    open(f'{student_answer_path}', 'r', encoding='utf-8') as student_answer_file:\n",
    "    question = question_file.read()  \n",
    "    ground_truth_answer = ground_truth_answer_file.read()\n",
    "    explanation_of_ground_truth_answer = explanation_of_ground_truth_answer_file.read()\n",
    "    few_shot = json.load(few_shot_file)\n",
    "    student_answer = student_answer_file.read()\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"qwen3-32b\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"ç°åœ¨ä½ æ˜¯ä¸€ä¸ªä¸­å­¦è€å¸ˆï¼Œä½ è¦è´Ÿè´£æ‰¹æ”¹ä½ å­¦ç”Ÿçš„ç‰©ç†è¯•å·çš„é¢˜ç›®ï¼Œ\n",
    "            ä¸»ä»»è¦æ±‚ä½ ä¸¥æ ¼æŒ‰ç…§é¢˜ç›®è§£æä¸å„ç‚¹è¯„åˆ†æ ‡å‡†é‡Œçš„è¸©åˆ†ç‚¹æ¥è¿›è¡Œæ‰¹æ”¹å¾—åˆ†ï¼Œ\n",
    "            å¹¶å¯¹æ¯ä¸€ä¸ªè¸©åˆ†ç‚¹è¿›è¡Œè§£é‡Šï¼Œæ¯”å¦‚è¯´è¿™ä¸ªè¸©åˆ†ç‚¹æœ‰å¯¹åº”çš„å…¬å¼ï¼Œå¾—åˆ°ç›¸åº”çš„åˆ†æ•°ï¼Œ\n",
    "            é‚£ä¸ªè¸©åˆ†ç‚¹æ²¡æœ‰å…¬å¼æˆ–è€…å…¬å¼é”™è¯¯ï¼Œä¸å¾—åˆ†\"ã€‚æŒ‰ç…§æ‰¹æ”¹ç¤ºä¾‹é‡Œçš„â€œfew_shot_outputâ€çš„jsonæ ¼å¼è¾“å‡º\n",
    "            é¢˜ç›®ï¼š{question}\n",
    "            æ ‡å‡†ç­”æ¡ˆï¼š{ground_truth_answer}\n",
    "            é¢˜ç›®è§£æä¸å„ç‚¹è¯„åˆ†æ ‡å‡†ï¼š{explanation_of_ground_truth_answer}\n",
    "            æ‰¹æ”¹ç¤ºä¾‹ï¼š{few_shot}\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"å­¦ç”Ÿç­”æ¡ˆï¼š{student_answer}\", \n",
    "        },\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    extra_body={\"enable_thinking\": False}\n",
    ")\n",
    "\n",
    "json_string = completion.choices[0].message.content\n",
    "json_string = json.loads(json_string)\n",
    "print(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æœ¬åœ°éƒ¨ç½²æ¨¡å‹æ¥ç®¡å…¨æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­¥éª¤ 1ï¼šå‘å‡ºè¯·æ±‚\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "question_path = \"question.txt\"\n",
    "ground_truth_answer_path = \"ground_truth_answer.txt\"\n",
    "explanation_of_ground_truth_answer_path = \"explanation.txt\"\n",
    "few_shot_path = \"few_shot.json\"\n",
    "student_answer_path = \"student_answer.txt\"\n",
    "\n",
    "with open(f'{question_path}', 'r', encoding='utf-8') as question_file,\\\n",
    "    open(f'{ground_truth_answer_path}', 'r', encoding='utf-8') as ground_truth_answer_file,\\\n",
    "    open(f'{explanation_of_ground_truth_answer_path}', 'r', encoding='utf-8') as explanation_of_ground_truth_answer_file,\\\n",
    "    open(f'{few_shot_path}', 'r', encoding='utf-8') as few_shot_file,\\\n",
    "    open(f'{student_answer_path}', 'r', encoding='utf-8') as student_answer_file:\n",
    "    question = question_file.read()  \n",
    "    ground_truth_answer = ground_truth_answer_file.read()\n",
    "    explanation_of_ground_truth_answer = explanation_of_ground_truth_answer_file.read()\n",
    "    few_shot = json.load(few_shot_file)\n",
    "    student_answer = student_answer_file.read()\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    #api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"http://127.0.0.1:11434/v1\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"qwen3:8b\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"ç°åœ¨ä½ æ˜¯ä¸€ä¸ªä¸­å­¦è€å¸ˆï¼Œä½ è¦è´Ÿè´£æ‰¹æ”¹ä½ å­¦ç”Ÿçš„ç‰©ç†è¯•å·çš„é¢˜ç›®ï¼Œ\n",
    "            ä¸»ä»»è¦æ±‚ä½ ä¸¥æ ¼æŒ‰ç…§é¢˜ç›®è§£æä¸å„ç‚¹è¯„åˆ†æ ‡å‡†é‡Œçš„è¸©åˆ†ç‚¹æ¥è¿›è¡Œæ‰¹æ”¹å¾—åˆ†ï¼Œ\n",
    "            å¹¶å¯¹æ¯ä¸€ä¸ªè¸©åˆ†ç‚¹è¿›è¡Œè§£é‡Šï¼Œæ¯”å¦‚è¯´è¿™ä¸ªè¸©åˆ†ç‚¹æœ‰å¯¹åº”çš„å…¬å¼ï¼Œå¾—åˆ°ç›¸åº”çš„åˆ†æ•°ï¼Œ\n",
    "            é‚£ä¸ªè¸©åˆ†ç‚¹æ²¡æœ‰å…¬å¼æˆ–è€…å…¬å¼é”™è¯¯ï¼Œä¸å¾—åˆ†\"ã€‚æŒ‰ç…§æ‰¹æ”¹ç¤ºä¾‹é‡Œçš„â€œfew_shot_outputâ€çš„jsonæ ¼å¼è¾“å‡º\n",
    "            é¢˜ç›®ï¼š{question}\n",
    "            æ ‡å‡†ç­”æ¡ˆï¼š{ground_truth_answer}\n",
    "            é¢˜ç›®è§£æä¸å„ç‚¹è¯„åˆ†æ ‡å‡†ï¼š{explanation_of_ground_truth_answer}\n",
    "            æ‰¹æ”¹ç¤ºä¾‹ï¼š{few_shot}\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"å­¦ç”Ÿç­”æ¡ˆï¼š{student_answer}\", \n",
    "        },\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    extra_body={\"enable_thinking\": False}\n",
    ")\n",
    "\n",
    "json_string = completion.choices[0].message.content\n",
    "json_string = json.loads(json_string)\n",
    "print(json_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
